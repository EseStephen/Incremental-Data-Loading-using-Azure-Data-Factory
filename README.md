# Azure-DataFactory-SCD-Type1
This project demonstrates how to implement Slowly Changing Dimension (SCD) Type 1 using Azure Data Factory (ADF) Data Flows.

ğŸš€ Project Overview
This project demonstrates how to implement Slowly Changing Dimension (SCD) Type 1 using Azure Data Factory (ADF) Data Flows. The project involves:<br>
Setting up an Azure Blob Storage container to store employee datasets in CSV format.<br>
Creating an Azure SQL Database to store employee records.<br>
Building an ADF pipeline to initially load employee data into SQL.<br>
Implementing SCD Type 1 using ADF Data Flows, which:<br>
Overwrites the salary of employees with updated salaries.<br>
Inserts new employees into the table.


ğŸ“Œ 1. Setting Up Azure Resources<br>
1ï¸âƒ£ Creating Azure Blob Storage<br>
Created a Storage Account in Azure.<br>
Created a Blob Container named employee-data.<br>
Uploaded the following files:<br>
Employees_Initial.csv (Initial employee data)<br>
Employees_Updated.csv (Updated employee data)


2ï¸âƒ£ Creating Azure SQL Database<br>
Created an Azure SQL Database named EmployeeDB.<br>
Created a table to store employee records using the following query:<br>
CREATE TABLE Employees (<br>
    EmployeeID INT PRIMARY KEY,<br>
    Name VARCHAR(100),<br>
    Salary DECIMAL(10,2),<br>
    Location VARCHAR(100)<br>
);


ğŸ“Œ 2. Setting Up Azure Data Factory (ADF)<br>
1ï¸âƒ£ Creating Linked Services<br>
In Azure Data Factory, I created two linked services:<br>
Azure Blob Storage Linked Service â€“ Connects to my storage account.<br>
Azure SQL Database Linked Service â€“ Connects to my SQL database.


2ï¸âƒ£ Creating Datasets<br>
Datasets help ADF read/write data from different sources. I created:<br>

ğŸ”¹ Datasets for Azure Blob Storage<br>
employeesource â†’ Points to Employees_Initial.csv in Blob Storage.<br>
employee2source â†’ Points to Employees_Updated.csv in Blob Storage.<br>
ğŸ”¹ Dataset for Azure SQL Database<br>
AzureSqlTableEmployee â†’ Points to the Employees table in SQL Database.


ğŸ“Œ 3. Loading Initial Employee Data into SQL<br>
Pipeline: LoadInitialEmployees<br>
Source â†’ employeesource (from Blob Storage).<br>
Sink â†’ AzureSqlTableEmployee (to SQL Database).<br>
Mapping â†’ Mapped columns:<br>
EmployeeID â†’ EmployeeID<br>
Name â†’ Name<br>
Salary â†’ Salary<br>
Location â†’ Location<br>
Executed Pipeline â†’ Successfully inserted records into the SQL database with copy data activity.


ğŸ“Œ 4. Implementing SCD Type 1 using ADF Data Flow<br>
ğŸ”¹ Step-by-Step Breakdown of the Data Flow<br>
I created a Data Flow named SCDType1DataFlow and added the following transformations:<br>
1ï¸âƒ£ Source: employee2source<br>
Reads the updated employee dataset (Employees_Updated.csv) from Blob Storage.<br>
2ï¸âƒ£ Source: AzureSqlTableEmployee<br>
Reads the existing employee records from the SQL Database.<br>
3ï¸âƒ£ Join Transformation (Left Outer Join)<br>
Primary Stream: Employees_Updated (Latest data).<br>
Lookup Stream: ExistingEmployees (Existing data).<br>
Join Condition: Employees_Updated.EmployeeID == ExistingEmployees.EmployeeID.<br>
This allows us to compare new data with existing records.<br>
4ï¸âƒ£ Derived Column Transformation<br>
Created two new columns:<br>
IsUpdated â†’ if(ExistingEmployees.Salary != Employees_Updated.Salary, 1, 0) (Checks if salary has changed).<br>
IsNew â†’ if(isNull(ExistingEmployees.EmployeeID), 1, 0) (Checks if the employee is new).<br>
5ï¸âƒ£ Filter Transformation<br>
Filter 1 (Updated Employees) â†’ IsUpdated == 1 (Extracts employees with salary updates).<br>
Filter 2 (New Employees) â†’ IsNew == 1 (Extracts new employees).<br>
6ï¸âƒ£ Alter Row Transformation<br>
Condition: Update if IsUpdated == 1.<br>
Condition: Insert if IsNew == 1.<br>
7ï¸âƒ£ Sink Transformation<br>
Sink 1 â†’ Updates employees in SQL Database.<br>
Sink 2 â†’ Inserts new employees into SQL Database.


ğŸ“Œ 5. Running & Validating the Pipeline<br>
Ran the Pipeline â†’ Successfully processed the updated employee data.<br>

